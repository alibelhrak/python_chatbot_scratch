{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYuR8nWdHscwSAJ6mwmhg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alibelhrak/python_chatbot_scratch/blob/main/Cha_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcfYeU7NB21O",
        "outputId": "46cd292e-ebdf-49a3-e622-432eae12230e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "o6844Q08WzFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "DGtDK_hq_dTk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0zUFHN4-jlX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Dropout   , Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing intentes"
      ],
      "metadata": {
        "id": "WFTtKoCy_nFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intents_data={\n",
        "  \"intents\": [\n",
        "    {\n",
        "      \"tag\": \"greeting\",\n",
        "      \"patterns\": [\"Hello\", \"Hi\", \"Hey\", \"What's up\", \"Good morning\"],\n",
        "      \"responses\": [\"Hello!\", \"Good day!\", \"Hey there!\", \"Hi! How can I assist you?\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"goodbye\",\n",
        "      \"patterns\": [\"Bye\", \"Goodbye\", \"See you later\", \"Take care\"],\n",
        "      \"responses\": [\"Goodbye!\", \"See you soon!\", \"Take care!\", \"Have a great day!\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"thanks\",\n",
        "      \"patterns\": [\"Thank you\", \"Thanks\", \"Appreciate it\"],\n",
        "      \"responses\": [\"You're welcome!\", \"Glad I could help!\", \"Anytime!\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"help\",\n",
        "      \"patterns\": [\"Can you help me?\", \"I need assistance\", \"Help me\"],\n",
        "      \"responses\": [\"Sure! What do you need help with?\", \"I'm here to assist!\", \"Tell me what you need help with.\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"weather\",\n",
        "      \"patterns\": [\"What's the weather like?\", \"Is it raining today?\", \"Tell me the forecast\"],\n",
        "      \"responses\": [\"I can check the weather for you!\", \"Do you want today's or this week's forecast?\", \"Let me get that info for you.\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"joke\",\n",
        "      \"patterns\": [\"Tell me a joke\", \"Make me laugh\", \"Do you know any jokes?\"],\n",
        "      \"responses\": [\"Why don’t skeletons fight each other? They don’t have the guts!\", \"Why did the scarecrow win an award? Because he was outstanding in his field!\"]\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AXbylGMR_q0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "vfuSRPsOLT41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words=[]\n",
        "classes=[]\n",
        "documents=[]\n",
        "ignore_symbols = ['.','?' , ';' , ':' , '!' , '%']\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "for intent in intents_data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        words_list = word_tokenize(pattern)\n",
        "        words.append(words_list)\n",
        "        documents.append((words_list, intent[\"tag\"]))\n",
        "\n",
        "        if intent[\"tag\"] not in classes:\n",
        "            classes.append(intent[\"tag\"])\n",
        "\n",
        "print(\"Documents:\", documents)"
      ],
      "metadata": {
        "id": "gT9fQVjTCIbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word) for sublist in words for word in sublist if word not in ignore_symbols]\n",
        "words = list(sorted(set(words)))\n",
        "classes = list(sorted(set(classes)))\n"
      ],
      "metadata": {
        "id": "Ik7DRreaWL0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saving files"
      ],
      "metadata": {
        "id": "8wIwHoSWXuax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('words.pkl' , 'wb') as f:\n",
        "  pickle.dump(words , f)\n",
        "\n",
        "with open('classes.pkl' , 'wb') as f:\n",
        "  pickle.dump(classes , f)"
      ],
      "metadata": {
        "id": "HtFtUJh5W1hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trianing Data"
      ],
      "metadata": {
        "id": "QOrBZOnLYu0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "  bag=[]\n",
        "  word_patterns = document[0]\n",
        "  word_patterns =[lemmatizer.lemmatize(word) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "  outputrow = list(output_empty)\n",
        "  outputrow[classes.index(document[1])] = 1\n",
        "  training.append([bag  , outputrow])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "train_x = list(training[: , 0])\n",
        "train_y = list(training[: , 1])"
      ],
      "metadata": {
        "id": "dTBC25H9YyXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "cRX2eQeGfF5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(len(train_x[0]),)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64 , activation = 'relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(len(train_y[0]) , activation = 'softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(optimizer = sgd ,  metrics=['accuracy'] , loss='categorical_crossentropy' )"
      ],
      "metadata": {
        "id": "VvAIDfWAfIm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(np.array(train_x) , np.array(train_y) , epochs=200 , batch_size =5 , verbose=0)"
      ],
      "metadata": {
        "id": "7zp0gTvpxd44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building ChatBot"
      ],
      "metadata": {
        "id": "_QauUOb8xvA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up(sentence):\n",
        "  sentence_word = word_tokenize(sentence)\n",
        "  sentence_word = [lemmatizer.lemmatize(word) for word in sentence_word if word not in ignore_symbols]\n",
        "  return sentence_word\n",
        "\n",
        "def bag_of_words(sentence):\n",
        "  sentence_word = clean_up(sentence)\n",
        "  bag=[0]* len(words)\n",
        "  for w in sentence_word:\n",
        "    for i, word in enumerate(words):\n",
        "      if word == w:\n",
        "        bag[i] =1\n",
        "  return np.array(bag)\n",
        "\n",
        "def predict_class(sentence):\n",
        "    bow = bag_of_words(sentence)\n",
        "    predicted_word = model.predict(np.array([bow]))[0]\n",
        "    ERROR_THRESHOLD = 0.8\n",
        "    results = [[i, r] for i, r in enumerate(predicted_word) if r > ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
        "\n",
        "    return return_list\n",
        "\n",
        "\n",
        "def get_responses(intent_list , intent_json):\n",
        "  tag = intent_list[0]['intent']\n",
        "  list_of_intents = intents_data['intents']\n",
        "  for i in list_of_intents:\n",
        "    if i['tag'] ==tag:\n",
        "      result =random.choice(i['responses'])\n",
        "  return result\n",
        "\n",
        "while True:\n",
        "    message = input(\"Vous: \")\n",
        "    ints = predict_class(message)\n",
        "    res = get_responses(ints, intents_data)\n",
        "    print(\"Chatbot:\", res)"
      ],
      "metadata": {
        "id": "qaRsKlcwxy0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3ZbXhf0EEU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}