{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZXbDMshDatM4ysOffuLEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alibelhrak/python_chatbot_scratch/blob/main/Cha_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcfYeU7NB21O",
        "outputId": "6c0a1c35-5732-4e38-e66c-36110e775e5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6844Q08WzFl",
        "outputId": "88e11005-218a-4cd8-835e-27ff5f31fd6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "DGtDK_hq_dTk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q0zUFHN4-jlX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Dropout   , Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing intentes"
      ],
      "metadata": {
        "id": "WFTtKoCy_nFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intents_data={\n",
        "  \"intents\": [\n",
        "    {\n",
        "      \"tag\": \"greeting\",\n",
        "      \"patterns\": [\"Hello\", \"Hi\", \"Hey\", \"What's up\", \"Good morning\"],\n",
        "      \"responses\": [\"Hello!\", \"Good day!\", \"Hey there!\", \"Hi! How can I assist you?\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"goodbye\",\n",
        "      \"patterns\": [\"Bye\", \"Goodbye\", \"See you later\", \"Take care\"],\n",
        "      \"responses\": [\"Goodbye!\", \"See you soon!\", \"Take care!\", \"Have a great day!\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"thanks\",\n",
        "      \"patterns\": [\"Thank you\", \"Thanks\", \"Appreciate it\"],\n",
        "      \"responses\": [\"You're welcome!\", \"Glad I could help!\", \"Anytime!\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"help\",\n",
        "      \"patterns\": [\"Can you help me?\", \"I need assistance\", \"Help me\"],\n",
        "      \"responses\": [\"Sure! What do you need help with?\", \"I'm here to assist!\", \"Tell me what you need help with.\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"weather\",\n",
        "      \"patterns\": [\"What's the weather like?\", \"Is it raining today?\", \"Tell me the forecast\"],\n",
        "      \"responses\": [\"I can check the weather for you!\", \"Do you want today's or this week's forecast?\", \"Let me get that info for you.\"]\n",
        "    },\n",
        "    {\n",
        "      \"tag\": \"joke\",\n",
        "      \"patterns\": [\"Tell me a joke\", \"Make me laugh\", \"Do you know any jokes?\"],\n",
        "      \"responses\": [\"Why don’t skeletons fight each other? They don’t have the guts!\", \"Why did the scarecrow win an award? Because he was outstanding in his field!\"]\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AXbylGMR_q0J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfuSRPsOLT41",
        "outputId": "44b97bcb-f0da-4868-d3c1-081464daae2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words=[]\n",
        "classes=[]\n",
        "documents=[]\n",
        "ignore_symbols = ['.','?' , ';' , ':' , '!' , '%']\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "for intent in intents_data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        words_list = word_tokenize(pattern)\n",
        "        words.append(words_list)\n",
        "        documents.append((words_list, intent[\"tag\"]))\n",
        "\n",
        "        if intent[\"tag\"] not in classes:\n",
        "            classes.append(intent[\"tag\"])\n",
        "\n",
        "print(\"Documents:\", documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT9fQVjTCIbP",
        "outputId": "6a7c81ca-3ec9-4257-8965-834a28cc311c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents: [(['Hello'], 'greeting'), (['Hi'], 'greeting'), (['Hey'], 'greeting'), (['What', \"'s\", 'up'], 'greeting'), (['Good', 'morning'], 'greeting'), (['Bye'], 'goodbye'), (['Goodbye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Take', 'care'], 'goodbye'), (['Thank', 'you'], 'thanks'), (['Thanks'], 'thanks'), (['Appreciate', 'it'], 'thanks'), (['Can', 'you', 'help', 'me', '?'], 'help'), (['I', 'need', 'assistance'], 'help'), (['Help', 'me'], 'help'), (['What', \"'s\", 'the', 'weather', 'like', '?'], 'weather'), (['Is', 'it', 'raining', 'today', '?'], 'weather'), (['Tell', 'me', 'the', 'forecast'], 'weather'), (['Tell', 'me', 'a', 'joke'], 'joke'), (['Make', 'me', 'laugh'], 'joke'), (['Do', 'you', 'know', 'any', 'jokes', '?'], 'joke')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word) for sublist in words for word in sublist if word not in ignore_symbols]\n",
        "words = list(sorted(set(words)))\n",
        "classes = list(sorted(set(classes)))\n"
      ],
      "metadata": {
        "id": "Ik7DRreaWL0l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saving files"
      ],
      "metadata": {
        "id": "8wIwHoSWXuax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('words.pkl' , 'wb') as f:\n",
        "  pickle.dump(words , f)\n",
        "\n",
        "with open('classes.pkl' , 'wb') as f:\n",
        "  pickle.dump(classes , f)"
      ],
      "metadata": {
        "id": "HtFtUJh5W1hg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trianing Data"
      ],
      "metadata": {
        "id": "QOrBZOnLYu0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "  bag=[]\n",
        "  word_patterns = document[0]\n",
        "  word_patterns =[lemmatizer.lemmatize(word) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "  outputrow = list(output_empty)\n",
        "  outputrow[classes.index(document[1])] = 1\n",
        "  training.append([bag  , outputrow])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "train_x = list(training[: , 0])\n",
        "train_y = list(training[: , 1])"
      ],
      "metadata": {
        "id": "dTBC25H9YyXw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "cRX2eQeGfF5o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VvAIDfWAfIm-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}